"""
Adapters - Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LoRA Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ².

ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°:
- LoRA (Low-Rank Adaptation)
- Bottleneck Adapters
- Prompt Tuning
"""

import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class BaseAdapter(ABC):
    """Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ²."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_active = False
    
    @abstractmethod
    def apply(self, model: Any) -> Any:
        """ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
        pass
    
    @abstractmethod
    def remove(self, model: Any) -> Any:
        """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
        pass


class LoRAAdapter(BaseAdapter):
    """
    Low-Rank Adaptation (LoRA) adapter.
    
    LoRA Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ trainable low-rank matrices Ğº frozen Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
    Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ fine-tuning Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².
    """
    
    def __init__(
        self,
        rank: int = 8,
        alpha: int = 32,
        target_modules: Optional[list] = None,
        dropout: float = 0.0
    ):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LoRA adapter.
        
        Args:
            rank: Ğ Ğ°Ğ½Ğ³ low-rank matrices
            alpha: Scaling parameter
            target_modules: ĞœĞ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LoRA
            dropout: Dropout rate
        """
        config = {
            'rank': rank,
            'alpha': alpha,
            'target_modules': target_modules or ['query', 'value'],
            'dropout': dropout
        }
        super().__init__(config)
        logger.info(f"âœ… LoRAAdapter initialized: rank={rank}, alpha={alpha}")
    
    def apply(self, model: Any) -> Any:
        """
        ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ LoRA Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
        
        TODO: Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ HuggingFace PEFT
        """
        logger.info(f"ğŸ”§ Applying LoRA adapter (rank={self.config['rank']})")
        # TODO: Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ PEFT
        self.is_active = True
        return model
    
    def remove(self, model: Any) -> Any:
        """Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ LoRA Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
        logger.info("ğŸ—‘ï¸ Removing LoRA adapter")
        self.is_active = False
        return model


class BottleneckAdapter(BaseAdapter):
    """
    Bottleneck Adapter - Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ feedforward ÑĞ»Ğ¾Ğ¸.
    """
    
    def __init__(self, bottleneck_size: int = 64, residual: bool = True):
        config = {
            'bottleneck_size': bottleneck_size,
            'residual': residual
        }
        super().__init__(config)
        logger.info(f"âœ… BottleneckAdapter initialized: size={bottleneck_size}")
    
    def apply(self, model: Any) -> Any:
        logger.info(f"ğŸ”§ Applying Bottleneck adapter (size={self.config['bottleneck_size']})")
        self.is_active = True
        return model
    
    def remove(self, model: Any) -> Any:
        logger.info("ğŸ—‘ï¸ Removing Bottleneck adapter")
        self.is_active = False
        return model
