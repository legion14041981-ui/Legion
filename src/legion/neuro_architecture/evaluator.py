"""
Multi-Objective Evaluator - Ð¼Ð½Ð¾Ð³Ð¾ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°.

ÐšÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸: accuracy, latency, cost, safety, robustness.
Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Pareto-Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑŽ + weighted scoring.
"""

import json
import logging
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñ‹."""
    proposal_id: str
    accuracy: float
    latency_ms: float
    resource_cost: float
    safety_score: float
    robustness_score: float
    composite_score: float
    rank: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), indent=2)


class MultiObjectiveEvaluator:
    """
    ÐœÐ½Ð¾Ð³Ð¾ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€.
    
    Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð° ÑÐºÐ¾Ñ€Ð¸Ð½Ð³Ð°:
    score = Î±*accuracy - Î²*latency - Î³*cost - Î´*safety_violations + Îµ*robustness
    """
    
    DEFAULT_WEIGHTS = {
        'accuracy': 0.5,
        'latency': 0.2,
        'cost': 0.15,
        'safety': 0.1,
        'robustness': 0.05
    }
    
    def __init__(self, weights: Optional[Dict[str, float]] = None):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ evaluator.
        
        Args:
            weights: Ð’ÐµÑÐ° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ñ
        """
        self.weights = weights or self.DEFAULT_WEIGHTS
        self._validate_weights()
        logger.info(f"âœ… MultiObjectiveEvaluator initialized with weights: {self.weights}")
    
    def _validate_weights(self) -> None:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾ Ð²ÐµÑÐ° ÑÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð² 1.0."""
        total = sum(self.weights.values())
        if abs(total - 1.0) > 0.01:
            logger.warning(f"âš ï¸ Weights sum to {total}, not 1.0. Normalizing...")
            factor = 1.0 / total
            self.weights = {k: v * factor for k, v in self.weights.items()}
    
    def evaluate(
        self,
        metrics_files: List[str],
        output_dir: str = "artifacts/evals"
    ) -> List[EvaluationResult]:
        """
        ÐžÑ†ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð°Ð±Ð¾Ñ€ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€.
        
        Args:
            metrics_files: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¿ÑƒÑ‚ÐµÐ¹ Ðº metrics.json
            output_dir: Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð»Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        
        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº EvaluationResult, Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ð¾ rank
        """
        logger.info(f"ðŸ“Š Evaluating {len(metrics_files)} architectures")
        
        results = []
        for metrics_file in metrics_files:
            result = self._evaluate_single(metrics_file)
            if result:
                results.append(result)
        
        # Ð Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        results = self._rank_results(results)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ
        self._save_results(results, output_dir)
        
        logger.info(f"âœ… Evaluation completed. Top-3 proposals: {[r.proposal_id for r in results[:3]]}")
        return results
    
    def _evaluate_single(self, metrics_file: str) -> Optional[EvaluationResult]:
        """ÐžÑ†ÐµÐ½Ð¸Ñ‚ÑŒ Ð¾Ð´Ð½Ñƒ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ."""
        try:
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            
            # Ð˜Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
            accuracy = metrics.get('eval_accuracy', 0.0)
            latency = metrics.get('latency_ms', 100.0)
            cost = metrics.get('gpu_memory_mb', 4000) / 1000  # GB
            
            # Safety Ð¸ robustness (Ð¿Ð¾ÐºÐ° Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ¸)
            safety = 1.0  # No violations
            robustness = 0.8  # Default
            
            # ÐšÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ‚Ð½Ñ‹Ð¹ ÑÐºÐ¾Ñ€
            composite = (
                self.weights['accuracy'] * accuracy -
                self.weights['latency'] * (latency / 100.0) -  # Normalize
                self.weights['cost'] * (cost / 10.0) -  # Normalize
                self.weights['safety'] * (1.0 - safety) * 10 +
                self.weights['robustness'] * robustness
            )
            
            return EvaluationResult(
                proposal_id=metrics.get('proposal_id', 'unknown'),
                accuracy=accuracy,
                latency_ms=latency,
                resource_cost=cost,
                safety_score=safety,
                robustness_score=robustness,
                composite_score=composite
            )
        except Exception as e:
            logger.error(f"âŒ Failed to evaluate {metrics_file}: {e}")
            return None
    
    def _rank_results(self, results: List[EvaluationResult]) -> List[EvaluationResult]:
        """Ð Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ composite score."""
        sorted_results = sorted(results, key=lambda x: x.composite_score, reverse=True)
        for i, result in enumerate(sorted_results, start=1):
            result.rank = i
        return sorted_results
    
    def _save_results(self, results: List[EvaluationResult], output_dir: str) -> None:
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        for result in results:
            result_file = output_path / f"{result.proposal_id}.json"
            with open(result_file, 'w') as f:
                f.write(result.to_json())
        
        # Summary file
        summary_file = output_path / "evaluation_summary.json"
        summary = {
            'total_evaluated': len(results),
            'top_3': [r.to_dict() for r in results[:3]],
            'weights_used': self.weights
        }
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.debug(f"ðŸ’¾ Evaluation results saved to {output_dir}")
