"""
Neuro-Learning Loop - Autonomous Self-Improvement System.

–¶–∏–∫–ª –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è:
1. Collect metrics
2. Analyze issues
3. Generate patches
4. Test & validate
5. Apply or rollback
6. Update registry
7. Sleep and repeat
"""

import asyncio
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import json

logger = logging.getLogger(__name__)


@dataclass
class MetricsSnapshot:
    """–°–Ω–∏–º–æ–∫ –º–µ—Ç—Ä–∏–∫ —Å–∏—Å—Ç–µ–º—ã."""
    timestamp: str
    error_rate: float
    latency_p50: float
    latency_p95: float
    latency_p99: float
    memory_mb: float
    cpu_percent: float
    cache_hit_rate: float
    agent_stability: float
    mobile_agent_success: float
    watchdog_alerts: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp,
            'error_rate': self.error_rate,
            'latency_p50': self.latency_p50,
            'latency_p95': self.latency_p95,
            'latency_p99': self.latency_p99,
            'memory_mb': self.memory_mb,
            'cpu_percent': self.cpu_percent,
            'cache_hit_rate': self.cache_hit_rate,
            'agent_stability': self.agent_stability,
            'mobile_agent_success': self.mobile_agent_success,
            'watchdog_alerts': self.watchdog_alerts
        }


@dataclass
class Issue:
    """–í—ã—è–≤–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞."""
    id: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    category: str  # 'performance', 'stability', 'logic', 'security'
    description: str
    affected_component: str
    metrics: Dict[str, float]
    detected_at: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.id,
            'severity': self.severity,
            'category': self.category,
            'description': self.description,
            'affected_component': self.affected_component,
            'metrics': self.metrics,
            'detected_at': self.detected_at
        }


@dataclass
class ImprovementPatch:
    """–ü–∞—Ç—á —É–ª—É—á—à–µ–Ω–∏—è."""
    id: str
    issue_id: str
    patch_type: str  # 'code_fix', 'config_tune', 'architecture_refactor'
    target_component: str
    changes: Dict[str, Any]
    expected_improvement: Dict[str, float]
    risk_score: float
    generated_at: str
    applied: bool = False
    rollback_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.id,
            'issue_id': self.issue_id,
            'patch_type': self.patch_type,
            'target_component': self.target_component,
            'changes': self.changes,
            'expected_improvement': self.expected_improvement,
            'risk_score': self.risk_score,
            'generated_at': self.generated_at,
            'applied': self.applied,
            'rollback_data': self.rollback_data
        }


class NeuroLearningLoop:
    """
    –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ü–∏–∫–ª —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏—è.
    
    Workflow:
    1. Collect current metrics
    2. Compare with baseline
    3. Identify issues
    4. Generate improvement patches
    5. Test patches in sandbox
    6. Apply if better, rollback if worse
    7. Update registry
    8. Sleep and repeat
    """
    
    def __init__(
        self,
        cycle_interval_hours: int = 12,
        metrics_storage: str = "artifacts/neuro_learning/metrics",
        patches_storage: str = "artifacts/neuro_learning/patches",
        enable_auto_apply: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Neuro-Learning Loop.
        
        Args:
            cycle_interval_hours: –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏ (—á–∞—Å—ã)
            metrics_storage: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
            patches_storage: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–∞—Ç—á–µ–π
            enable_auto_apply: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø–∞—Ç—á–∏
        """
        self.cycle_interval = timedelta(hours=cycle_interval_hours)
        self.metrics_storage = Path(metrics_storage)
        self.patches_storage = Path(patches_storage)
        self.enable_auto_apply = enable_auto_apply
        
        # Create directories
        self.metrics_storage.mkdir(parents=True, exist_ok=True)
        self.patches_storage.mkdir(parents=True, exist_ok=True)
        
        self.baseline_metrics: Optional[MetricsSnapshot] = None
        self.current_cycle = 0
        self.running = False
        
        logger.info(f"‚úÖ NeuroLearningLoop initialized")
        logger.info(f"   Cycle interval: {cycle_interval_hours}h")
        logger.info(f"   Auto-apply: {enable_auto_apply}")
    
    async def run(self) -> None:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.
        """
        self.running = True
        logger.info("üß¨ Starting Neuro-Learning Loop...")
        
        # Load baseline
        self.baseline_metrics = self._load_baseline()
        if not self.baseline_metrics:
            logger.info("üìä No baseline found, creating initial baseline...")
            self.baseline_metrics = await self._collect_metrics()
            self._save_baseline(self.baseline_metrics)
        
        while self.running:
            self.current_cycle += 1
            logger.info(f"\n{'='*60}")
            logger.info(f"üîÑ NEURO-LEARNING CYCLE #{self.current_cycle}")
            logger.info(f"{'='*60}")
            
            try:
                # Step 1: Collect metrics
                logger.info("\n[1/6] Collecting metrics...")
                current_metrics = await self._collect_metrics()
                self._save_metrics(current_metrics)
                
                # Step 2: Analyze issues
                logger.info("\n[2/6] Analyzing issues...")
                issues = await self._analyze_issues(current_metrics)
                logger.info(f"   Found {len(issues)} issues")
                
                if not issues:
                    logger.info("   ‚úÖ No issues detected, system healthy")
                    await self._sleep()
                    continue
                
                # Step 3: Generate patches
                logger.info("\n[3/6] Generating improvement patches...")
                patches = await self._generate_patches(issues)
                logger.info(f"   Generated {len(patches)} patches")
                
                # Step 4: Test patches
                logger.info("\n[4/6] Testing patches...")
                tested_patches = await self._test_patches(patches)
                approved = [p for p in tested_patches if p.get('approved', False)]
                logger.info(f"   {len(approved)}/{len(tested_patches)} patches approved")
                
                if not approved:
                    logger.info("   ‚ö†Ô∏è No patches approved, skipping apply")
                    await self._sleep()
                    continue
                
                # Step 5: Apply or rollback
                logger.info("\n[5/6] Applying patches...")
                if self.enable_auto_apply:
                    results = await self._apply_patches(approved)
                    logger.info(f"   Applied {results['applied']}/{len(approved)} patches")
                    logger.info(f"   Rolled back {results['rolled_back']} patches")
                else:
                    logger.info("   ‚ö†Ô∏è Auto-apply disabled, skipping")
                
                # Step 6: Update registry
                logger.info("\n[6/6] Updating registry...")
                await self._update_registry(current_metrics, issues, approved)
                logger.info("   ‚úÖ Registry updated")
                
            except Exception as e:
                logger.error(f"‚ùå Error in neuro-learning cycle: {e}")
                logger.exception(e)
            
            # Sleep until next cycle
            await self._sleep()
    
    async def _collect_metrics(self) -> MetricsSnapshot:
        """
        –°–æ–±—Ä–∞—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã.
        
        Returns:
            MetricsSnapshot
        """
        # TODO: Integrate with real monitoring
        # For now, return mock metrics
        return MetricsSnapshot(
            timestamp=datetime.utcnow().isoformat(),
            error_rate=0.02,
            latency_p50=45.2,
            latency_p95=89.5,
            latency_p99=142.3,
            memory_mb=1980.0,
            cpu_percent=28.5,
            cache_hit_rate=0.82,
            agent_stability=0.94,
            mobile_agent_success=0.68,
            watchdog_alerts=2
        )
    
    async def _analyze_issues(self, metrics: MetricsSnapshot) -> List[Issue]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤—ã—è–≤–ª—è—Ç—å –ø—Ä–æ–±–ª–µ–º—ã.
        
        Args:
            metrics: –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
        """
        issues = []
        
        # Check error rate
        if metrics.error_rate > 0.05:
            issues.append(Issue(
                id=f"issue_{int(time.time())}_error_rate",
                severity='high',
                category='stability',
                description=f"Error rate too high: {metrics.error_rate:.2%}",
                affected_component='core',
                metrics={'error_rate': metrics.error_rate},
                detected_at=datetime.utcnow().isoformat()
            ))
        
        # Check latency
        if self.baseline_metrics:
            latency_degradation = (
                metrics.latency_p95 - self.baseline_metrics.latency_p95
            ) / self.baseline_metrics.latency_p95
            
            if latency_degradation > 0.20:
                issues.append(Issue(
                    id=f"issue_{int(time.time())}_latency",
                    severity='medium',
                    category='performance',
                    description=f"Latency degraded by {latency_degradation:.1%}",
                    affected_component='core',
                    metrics={'latency_p95': metrics.latency_p95},
                    detected_at=datetime.utcnow().isoformat()
                ))
        
        # Check cache hit rate
        if metrics.cache_hit_rate < 0.75:
            issues.append(Issue(
                id=f"issue_{int(time.time())}_cache",
                severity='low',
                category='performance',
                description=f"Cache hit rate below target: {metrics.cache_hit_rate:.1%}",
                affected_component='storage',
                metrics={'cache_hit_rate': metrics.cache_hit_rate},
                detected_at=datetime.utcnow().isoformat()
            ))
        
        # Check mobile agent
        if metrics.mobile_agent_success < 0.80:
            issues.append(Issue(
                id=f"issue_{int(time.time())}_mobile",
                severity='medium',
                category='stability',
                description=f"Mobile agent success too low: {metrics.mobile_agent_success:.1%}",
                affected_component='mobile_agent',
                metrics={'mobile_agent_success': metrics.mobile_agent_success},
                detected_at=datetime.utcnow().isoformat()
            ))
        
        return issues
    
    async def _generate_patches(self, issues: List[Issue]) -> List[ImprovementPatch]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—á–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º.
        
        Args:
            issues: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ç—á–µ–π
        """
        patches = []
        
        for issue in issues:
            if issue.category == 'performance' and 'cache' in issue.affected_component:
                # Cache optimization patch
                patches.append(ImprovementPatch(
                    id=f"patch_{int(time.time())}_{issue.id}",
                    issue_id=issue.id,
                    patch_type='config_tune',
                    target_component='storage',
                    changes={
                        'cache_ttl': 3600,
                        'cache_max_size': 1500,  # Increase from 1000
                        'prefetch_enabled': True
                    },
                    expected_improvement={'cache_hit_rate': 0.10},
                    risk_score=0.1,
                    generated_at=datetime.utcnow().isoformat()
                ))
            
            elif issue.category == 'stability' and 'mobile' in issue.affected_component:
                # Mobile agent improvement patch
                patches.append(ImprovementPatch(
                    id=f"patch_{int(time.time())}_{issue.id}",
                    issue_id=issue.id,
                    patch_type='code_fix',
                    target_component='mobile_agent',
                    changes={
                        'max_retries': 5,  # Increase from 3
                        'retry_delay': 2.0,
                        'self_healing_enabled': True,
                        'error_recovery_strategies': 5  # More strategies
                    },
                    expected_improvement={'mobile_agent_success': 0.15},
                    risk_score=0.3,
                    generated_at=datetime.utcnow().isoformat()
                ))
        
        return patches
    
    async def _test_patches(self, patches: List[ImprovementPatch]) -> List[Dict[str, Any]]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—á–∏.
        
        Args:
            patches: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—á–µ–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        results = []
        
        for patch in patches:
            # TODO: Implement real testing
            # For now, approve low-risk patches
            approved = patch.risk_score < 0.5
            
            results.append({
                'patch': patch,
                'approved': approved,
                'test_results': {
                    'static_analysis': 'passed',
                    'dynamic_evaluation': 'passed' if approved else 'failed',
                    'integration_tests': 'passed' if approved else 'skipped'
                }
            })
        
        return results
    
    async def _apply_patches(self, approved_patches: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –æ–¥–æ–±—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—á–∏.
        
        Args:
            approved_patches: –°–ø–∏—Å–æ–∫ –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—á–µ–π
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        """
        applied = 0
        rolled_back = 0
        
        for result in approved_patches:
            patch = result['patch']
            
            try:
                # TODO: Implement real patch application
                logger.info(f"   Applying patch {patch.id}...")
                
                # Save patch
                patch_file = self.patches_storage / f"{patch.id}.json"
                patch_file.write_text(json.dumps(patch.to_dict(), indent=2))
                
                applied += 1
                
            except Exception as e:
                logger.error(f"   ‚ùå Failed to apply patch {patch.id}: {e}")
                rolled_back += 1
        
        return {'applied': applied, 'rolled_back': rolled_back}
    
    async def _update_registry(self, metrics: MetricsSnapshot, issues: List[Issue], patches: List[Dict[str, Any]]) -> None:
        """
        –û–±–Ω–æ–≤–∏—Ç—å registry –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞.
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏
            issues: –ü—Ä–æ–±–ª–µ–º—ã
            patches: –ü–∞—Ç—á–∏
        """
        # TODO: Integrate with ArchitectureRegistry
        cycle_summary = {
            'cycle': self.current_cycle,
            'timestamp': datetime.utcnow().isoformat(),
            'metrics': metrics.to_dict(),
            'issues_detected': len(issues),
            'patches_applied': len(patches)
        }
        
        summary_file = self.metrics_storage / f"cycle_{self.current_cycle}.json"
        summary_file.write_text(json.dumps(cycle_summary, indent=2))
    
    def _load_baseline(self) -> Optional[MetricsSnapshot]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å baseline metrics."""
        baseline_file = self.metrics_storage / "baseline.json"
        if not baseline_file.exists():
            return None
        
        data = json.loads(baseline_file.read_text())
        return MetricsSnapshot(**data)
    
    def _save_baseline(self, metrics: MetricsSnapshot) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å baseline metrics."""
        baseline_file = self.metrics_storage / "baseline.json"
        baseline_file.write_text(json.dumps(metrics.to_dict(), indent=2))
    
    def _save_metrics(self, metrics: MetricsSnapshot) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–µ metrics."""
        metrics_file = self.metrics_storage / f"metrics_{int(time.time())}.json"
        metrics_file.write_text(json.dumps(metrics.to_dict(), indent=2))
    
    async def _sleep(self) -> None:
        """Sleep –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞."""
        next_cycle = datetime.utcnow() + self.cycle_interval
        logger.info(f"\nüò¥ Sleeping until next cycle: {next_cycle.isoformat()}")
        logger.info(f"   Next cycle in {self.cycle_interval.total_seconds() / 3600:.1f} hours")
        
        await asyncio.sleep(self.cycle_interval.total_seconds())
    
    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ü–∏–∫–ª."""
        logger.info("üõë Stopping Neuro-Learning Loop...")
        self.running = False
